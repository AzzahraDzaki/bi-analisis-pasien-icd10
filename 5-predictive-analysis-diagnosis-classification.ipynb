{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "767b0315",
   "metadata": {},
   "source": [
    "This file implements a classification model using XGBoost to predict patient diagnoses based on demographic and administrative features. This model was developed to help the hospital identify patient profiles with a high risk for specific diagnoses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68dd832b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, roc_curve, auc\n",
    "from itertools import cycle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932a3a7f",
   "metadata": {},
   "source": [
    "Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd05d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for classification\n",
    "# Focus on the 5 diagnoses with the highest historical support\n",
    "top5_icd10 = ['A09.9', 'S09.8', 'Z03.8', 'I64', 'E11.2']\n",
    "df_inap_classif = df_rawatinap[df_rawatinap['icd_10'].isin(top5_icd10)].copy()\n",
    "\n",
    "# Feature Engineering\n",
    "# Create new features from date data\n",
    "df_inap_classif['bulan_admisi'] = pd.to_datetime(df_inap_classif['tanggal_admisi'])\n",
    "df_inap_classif['bulan_num'] = df_inap_classif['bulan_admisi'].dt.month\n",
    "df_inap_classif['tahun_num'] = df_inap_classif['bulan_admisi'].dt.year\n",
    "df_inap_classif['is_weekend'] = df_inap_classif['bulan_admisi'].dt.weekday.isin([5,6]).astype(int)\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "X = df_inap_classif[[\n",
    "    \"umur\", \"lama_rawat\", \"jenis_kelamin\", \"cara_bayar\",\n",
    "    \"status_disposisi\", \"tindakan\", \"bulan_num\", \"tahun_num\", \"is_weekend\"\n",
    "]]\n",
    "y = df_inap_classif[\"icd_10\"]\n",
    "\n",
    "# Convert target labels to a numerical format for the model\n",
    "le_icd = LabelEncoder()\n",
    "y_encoded = le_icd.fit_transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b46998",
   "metadata": {},
   "source": [
    "Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec5d6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training (80%) and testing (20%) sets\n",
    "# Use stratification to maintain class distribution\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded, test_size=0.2, stratify=y_encoded, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf990ca",
   "metadata": {},
   "source": [
    "Model Training & Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715590a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define categorical and numerical features\n",
    "numeric_features = [\"umur\", \"lama_rawat\", \"bulan_num\", \"tahun_num\", \"is_weekend\"]\n",
    "categorical_features = [\"jenis_kelamin\", \"cara_bayar\", \"status_disposisi\", \"tindakan\"]\n",
    "\n",
    "# Preprocessor for OneHotEncoding on categorical features\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_features)\n",
    "], remainder='passthrough')\n",
    "\n",
    "# Use a Pipeline to combine preprocessing and the model\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', XGBClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# --- Assumption: Hyperparameter tuning was performed to find the best parameters ---\n",
    "# The full tuning code is available in the technical documentation (README)\n",
    "best_params = {\n",
    "    'classifier__learning_rate': 0.05,\n",
    "    'classifier__max_depth': 6,\n",
    "    'classifier__n_estimators': 500,\n",
    "    'classifier__subsample': 0.8,\n",
    "    'classifier__colsample_bytree': 0.8\n",
    "}\n",
    "\n",
    "# Train the model using the best parameters\n",
    "pipeline.set_params(**best_params)\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2182e1",
   "metadata": {},
   "source": [
    "Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab8b026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test data\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Calculate key evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average=\"weighted\", zero_division=0)\n",
    "recall = recall_score(y_test, y_pred, average=\"weighted\", zero_division=0)\n",
    "f1 = f1_score(y_test, y_pred, average=\"weighted\", zero_division=0)\n",
    "\n",
    "# Display evaluation results\n",
    "print(\"=== XGBoost Classification Model Evaluation ===\")\n",
    "print(f\"Accuracy : {accuracy:.3f}\")\n",
    "print(f\"Precision: {precision:.3f}\")\n",
    "print(f\"Recall   : {recall:.3f}\")\n",
    "print(f\"F1-score : {f1:.3f}\")\n",
    "\n",
    "# Display a more detailed classification report\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(classification_report(y_test, y_pred, target_names=le_icd.classes_, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369111e4",
   "metadata": {},
   "source": [
    "Visualization (ROC Curve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a316eec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate prediction probabilities\n",
    "y_score = pipeline.predict_proba(X_test)\n",
    "y_test_binarized = pd.get_dummies(y_test)\n",
    "n_classes = y_test_binarized.shape[1]\n",
    "\n",
    "# Calculate the ROC curve and AUC for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test_binarized.iloc[:, i], y_score[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Visualize the ROC curve\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.figure(figsize=(10, 8))\n",
    "colors = cycle(['#FF5733', '#33FF57', '#3357FF', '#FF33A1', '#A133FF'])\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=2,\n",
    "             label=f'ROC curve of class {le_icd.classes_[i]} (area = {roc_auc[i]:.2f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2, label='Random Guessing (AUC = 0.50)')\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
